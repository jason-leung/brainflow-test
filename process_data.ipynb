{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0fcdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f04187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, pyxdf, json, yaml\n",
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from mne.time_frequency import psd_welch, tfr_morlet, tfr_multitaper\n",
    "# from multitaper_spectrogram_python import multitaper_spectrogram\n",
    "from mne.decoding import Scaler, Vectorizer\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, HalvingGridSearchCV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a0ee15",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db2e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_file = 'config_MI-hands.yaml'\n",
    "\n",
    "with open(config_file) as f:\n",
    "    config = yaml.load(f.read(), Loader=yaml.Loader)\n",
    "    print(config)\n",
    "    locals().update(config)\n",
    "\n",
    "lslDir = os.path.join(os.path.expanduser('~'), 'Documents\\CurrentStudy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df62ee7",
   "metadata": {},
   "source": [
    "### Find LSL Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93040a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find files\n",
    "xdf_files = []\n",
    "hasSubject = subject!=''\n",
    "hasSession = session!=''\n",
    "hasTask = task!=''\n",
    "for root, dir, files in os.walk(lslDir):\n",
    "    for file in files:\n",
    "        validFile = True\n",
    "        if hasSubject:\n",
    "            validFile = validFile and (('sub-'+subject) in file)\n",
    "        if hasSession:\n",
    "            validFile = validFile and (('ses-S' + str(session).zfill(3)) in file)\n",
    "        if hasTask:\n",
    "            validFile = validFile and (('task-' + task) in file)\n",
    "        validFile = validFile and file.endswith('.xdf')\n",
    "        if validFile:\n",
    "            print(file)\n",
    "            matchingFile = os.path.join(root, file)\n",
    "            xdf_files.append(matchingFile)\n",
    "\n",
    "if len(xdf_files) == 0:\n",
    "    print('No files found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082bb234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parse streams\n",
    "eeg_stream, marker_stream = [], []\n",
    "\n",
    "print('Parsing streams')\n",
    "for xdf_file in xdf_files:\n",
    "    streams, header = pyxdf.load_xdf(xdf_file)\n",
    "    for i in range(len(streams)):\n",
    "        if streams[i]['info']['type'][0] == eeg_stream_type:\n",
    "            print(\"Found %s stream in %s\" % (eeg_stream_type, os.path.basename(xdf_file)))\n",
    "            eeg_stream.append(streams[i])\n",
    "        elif streams[i]['info']['type'][0] == markers_stream_type:\n",
    "            print(\"Found %s stream in %s\" % (markers_stream_type, os.path.basename(xdf_file)))\n",
    "            marker_stream.append(streams[i])\n",
    "del streams, header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af60e1",
   "metadata": {},
   "source": [
    "### Extract EEG and Marker data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b6f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract EEG Info\n",
    "print(\"Extracting EEG info...\")\n",
    "\n",
    "ch_names = []\n",
    "if eeg_stream[0]['info']['desc'][0]:\n",
    "    print(\"EEG channel names found\")\n",
    "    for i in range(len(eeg_stream[0]['info']['desc'][0]['channels'][0]['channel'])):\n",
    "        ch_names.append(eeg_stream[0]['info']['desc'][0]['channels'][0]['channel'][i]['label'][0])\n",
    "else:\n",
    "    ch_names = default_ch_names\n",
    "print('Channels: ', ch_names)\n",
    "\n",
    "sfreq = float(eeg_stream[0]['info']['nominal_srate'][0])\n",
    "print('Sampling frequency: ', sfreq)\n",
    "\n",
    "# Create MNE info object\n",
    "eeg_info = mne.create_info(ch_names, sfreq, ch_types='eeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f764ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Montage\n",
    "montage = mne.channels.read_custom_montage(montage_file)\n",
    "# montage.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8048953f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all EEG data\n",
    "eeg_raw_list = []\n",
    "\n",
    "for n in range(len(eeg_stream)):\n",
    "    # Create MNE Raw object\n",
    "    eeg_data = np.transpose(eeg_stream[n]['time_series'])\n",
    "    eeg_data = eeg_data / 1e6\n",
    "    print(eeg_data.shape)\n",
    "    eeg_raw = mne.io.RawArray(eeg_data, eeg_info)\n",
    "    \n",
    "    # Set montage\n",
    "    eeg_raw = eeg_raw.set_montage(montage)\n",
    "\n",
    "    # Add annotations\n",
    "    onset, duration, description = [], [], []\n",
    "    current_target = -1\n",
    "    current_flash = -1\n",
    "    for i in range(len(marker_stream[n]['time_series'])):\n",
    "        if 'MI' in task:\n",
    "            if ('rest' in marker_stream[n]['time_series'][i][0]) and ('cue' not in marker_stream[n]['time_series'][i][0]):\n",
    "                window_onset = tmin\n",
    "                window_start = marker_stream[n]['time_stamps'][i] - eeg_stream[n]['time_stamps'][0]\n",
    "                while (window_onset + window_size <= tmax):\n",
    "                    onset.append(window_start + window_onset)\n",
    "                    duration.append(window_size)\n",
    "                    description.append(marker_stream[n]['time_series'][i][0])\n",
    "                    window_onset = window_onset + window_size - window_overlap\n",
    "            elif ('task' in marker_stream[n]['time_series'][i][0]) and ('cue' not in marker_stream[n]['time_series'][i][0]):\n",
    "                window_onset = tmin\n",
    "                window_start = marker_stream[n]['time_stamps'][i] - eeg_stream[n]['time_stamps'][0]\n",
    "                while (window_onset + window_size <= tmax):\n",
    "                    onset.append(window_start + window_onset)\n",
    "                    duration.append(window_size)\n",
    "                    description.append(marker_stream[n]['time_series'][i][0].replace('task_', '').replace('-','/'))\n",
    "                    window_onset = window_onset + window_size - window_overlap\n",
    "        elif 'P300' in task:\n",
    "            if('target' in marker_stream[n]['time_series'][i][0]):\n",
    "                current_target = json.loads(marker_stream[n]['time_series'][i][0])['target']\n",
    "            elif('flash' in marker_stream[n]['time_series'][i][0]):\n",
    "                current_flash = json.loads(marker_stream[n]['time_series'][i][0])['flash']\n",
    "                onset.append(marker_stream[n]['time_stamps'][i] - eeg_stream[n]['time_stamps'][0])\n",
    "                duration.append(task_duration)\n",
    "                description.append(\"target\" if current_flash == current_target else \"nontarget\")\n",
    "    annotations = mne.Annotations(onset, duration, description)\n",
    "    eeg_raw = eeg_raw.set_annotations(annotations)\n",
    "    \n",
    "    # Create list of raw objects\n",
    "    eeg_raw_list.append(eeg_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea700653",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eeg_raw_list[0].annotations.orig_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8688d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Concatenate raw objects\n",
    "raw = mne.concatenate_raws(eeg_raw_list)\n",
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e81434",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91852a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common average reference\n",
    "raw_orig = raw.copy()\n",
    "raw = raw.set_eeg_reference('average', projection=False)\n",
    "\n",
    "if plotGraphs:\n",
    "    fig = raw_orig.plot(title='Before Re-referencing', n_channels=16, scalings=scalings)\n",
    "    fig = raw.plot(title='After Re-referencing', n_channels=16, scalings=scalings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0930ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandpass filter data\n",
    "raw_orig = raw.copy()\n",
    "raw = raw.filter(l_freq=bp_l_freq, h_freq=bp_h_freq)\n",
    "\n",
    "if plotGraphs:\n",
    "    fig = raw_orig.plot(title='Before Filtering', scalings=scalings, duration=plot_duration)\n",
    "    fig = raw.plot(title='After Filtering', scalings=scalings, duration=plot_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b855a6",
   "metadata": {},
   "source": [
    "### ICA Artifact Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b190f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if performICA:\n",
    "    print('Performing ICA artifact removal...')\n",
    "    raw_orig = raw.copy()\n",
    "\n",
    "    # filter data to remove slow drifts\n",
    "    raw_filt = raw.copy()\n",
    "    raw_filt.filter(l_freq=1., h_freq=None)\n",
    "\n",
    "    # ICA decomposition\n",
    "    ica = mne.preprocessing.ICA(n_components=16, method='fastica', max_iter=200, random_state=42, verbose=True)\n",
    "    ica = ica.fit(raw_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1828eae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ICA sources\n",
    "if performICA and plotGraphs:\n",
    "    fig = ica.plot_sources(raw_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb69e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select source that corresponds to artifact and remove it\n",
    "if performICA:\n",
    "    ica.exclude = [0]\n",
    "    # ica.exclude = [2]\n",
    "    print('ICA sources to exclude: ', ica.exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f41cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if performICA:\n",
    "    ica.apply(raw)\n",
    "    if plotGraphs:\n",
    "        fig = raw_orig.plot(title='Before ICA', scalings=scalings, duration=plot_duration)\n",
    "        fig = raw.plot(title='After ICA', scalings=scalings, duration=plot_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca8d881",
   "metadata": {},
   "source": [
    "### Epoch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c88f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch data\n",
    "events, event_id = mne.events_from_annotations(raw, event_id=event_dict)\n",
    "epochs = mne.Epochs(raw, events, event_id=event_id, tmin=0., tmax=window_size, baseline=None, picks='eeg', preload=True)\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0e4a0e",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806fe2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "y = epochs.events[:,-1] - min(epochs.events[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974006a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "# Time-Domain Features\n",
    "if features == 'time':\n",
    "    scaler = Scaler(epochs.info)\n",
    "    X = scaler.fit_transform(epochs.get_data())\n",
    "    \n",
    "    if ('P300' in task) and plotGraphs:\n",
    "        fig = epochs['target'].average().plot_joint(times=[-0.2, 0., 0.3, 0.7], ts_args=dict(ylim=dict(eeg=[-10, 10])))\n",
    "        fig = epochs['nontarget'].average().plot_joint(times=[-0.2, 0., 0.3, 0.7], ts_args=dict(ylim=dict(eeg=[-10, 10])))\n",
    "#         fig = epoch['target'].average().plot_joint(times=[-0.2, 0., 0.3, 0.7], picks=['Fz','Cz','P3','Pz','P4','PO3','PO4','Oz'], ts_args=dict(ylim=dict(eeg=[-10, 10])))\n",
    "#         fig = epoch['nontarget'].average().plot_joint(times=[-0.2, 0., 0.3, 0.7], picks=['Fz','Cz','P3','Pz','P4','PO3','PO4','Oz'], ts_args=dict(ylim=dict(eeg=[-10, 10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db58950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Frequency-Domain Features\n",
    "if features == 'psd':\n",
    "    psds, freqs = psd_welch(epochs, average='mean', fmin=bp_l_freq, fmax=bp_h_freq, n_fft=126, n_jobs=-1)\n",
    "    X = 10 * np.log10(psds)\n",
    "#     X = psds / np.sum(psds, axis=-1, keepdims=True)\n",
    "    \n",
    "    if ('MI' in task) and plotGraphs:\n",
    "#         sel_chs = [2, 3, 4, 5, 6, 7, 14, 15]\n",
    "        sel_chs = range(16)\n",
    "        psd_means_class_0 = np.transpose(np.mean(X[y==0], axis=0))\n",
    "        psd_means_class_1 = np.transpose(np.mean(X[y==1], axis=0))\n",
    "        psd_means_class_0 = psd_means_class_0[:,sel_chs]\n",
    "        psd_means_class_1 = psd_means_class_1[:,sel_chs]\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        for i in range(len(sel_chs)):\n",
    "            line = ax.plot(freqs, psd_means_class_0[:,i], ':', label=ch_names[sel_chs[i]] + ' Rest')\n",
    "            ax.plot(freqs, psd_means_class_1[:,i], '-', label=ch_names[sel_chs[i]] + ' MI-hands', color=line[0].get_color())\n",
    "        ax.set(title='Welch PSD', xlabel='Frequency (Hz)', ylabel='Power Spectral Density (dB)')\n",
    "        ax.set_ylim(bottom=-135, top=-85)\n",
    "        ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d054eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-frequency features\n",
    "if features =='tfr':\n",
    "    # Parameters\n",
    "    freqs = np.logspace(*np.log10([6, 35]), num=8)\n",
    "#     freqs = np.linspace(2, 40, 20)\n",
    "    print('TFR freqs: ', freqs)\n",
    "    n_cycles = freqs / 2.\n",
    "    time_bandwidth = 4.0 # param for multitaper\n",
    "    \n",
    "    # Compute TFR Power\n",
    "    if tfr_type == 'morlet':\n",
    "        power = tfr_morlet(epochs, freqs, n_cycles=n_cycles, use_fft=False, return_itc=False, average=False, n_jobs=-1)\n",
    "    elif tfr_type == 'multitaper':\n",
    "        power = tfr_multitaper(epochs, freqs, n_cycles=n_cycles, time_bandwidth=time_bandwidth, use_fft=False, return_itc=False, average=False, n_jobs=-1)\n",
    "    print(power.data.shape)\n",
    "    X = power.data\n",
    "    \n",
    "    if ('MI' in task) and plotGraphs:\n",
    "        fig = power['rest'].average().plot_topo(baseline=baseline, mode='percent', cmap='jet', tmin=tmin, tmax=tmax, vmin=vmin, vmax=vmax, title='Average power (Rest)')\n",
    "        fig = power['MI/hands'].average().plot_topo(baseline=baseline, mode='percent', cmap='jet', tmin=tmin, tmax=tmax, vmin=vmin, vmax=vmax, title='Average power (MI-hands)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76871933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plt.close('all')\n",
    "# baseline = (0., 0.1)\n",
    "# vmin, vmax = -1, 1\n",
    "# fig = power['rest'].average().plot_topo(baseline=baseline, mode='percent', tmin=tmin, tmax=tmax, vmin=vmin, vmax=vmax, title='Average power (Rest)')\n",
    "# fig = power['MI/hands'].average().plot_topo(baseline=baseline, mode='percent', tmin=tmin, tmax=tmax, vmin=vmin, vmax=vmax, title='Average power (MI-hands)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select Channels\n",
    "# print('Selecting channels from data...')\n",
    "# print('Original X.shape: ', X.shape)\n",
    "# X = X[:,2:, :]\n",
    "# print('X.shape: ', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adf9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize features\n",
    "if len(X.shape) > 2:\n",
    "    print('Vectorizing features to 2D...')\n",
    "    print('Original X.shape: ', X.shape)\n",
    "    vec = Vectorizer()\n",
    "    X = vec.fit_transform(X)\n",
    "\n",
    "print('X.shape: ', X.shape)\n",
    "print('y.shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59010a6",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4968dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cross validation\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "# Set up scoring\n",
    "scoring = 'accuracy'\n",
    "scores = {'Classifier': [],\n",
    "          'Score': [],\n",
    "          'Std': []\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47498dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Classifiers\n",
    "classifiers = []\n",
    "\n",
    "# KNN\n",
    "params = {}\n",
    "params['n_neighbors'] = np.arange(2,11,1)\n",
    "params['weights'] = ['uniform', 'distance']\n",
    "clf = HalvingGridSearchCV(KNeighborsClassifier(), param_grid=params, n_jobs=-1, cv=cv, scoring=scoring)\n",
    "classifiers.append(['KNN', clf, params])\n",
    "\n",
    "# DT\n",
    "params = {}\n",
    "params['criterion'] = ['gini', 'entropy']\n",
    "params['min_samples_split'] = np.arange(2,11,2)\n",
    "clf = HalvingGridSearchCV(DecisionTreeClassifier(), param_grid=params, n_jobs=-1, cv=cv, scoring=scoring)\n",
    "classifiers.append(['DT', clf, params])\n",
    "\n",
    "# RF\n",
    "params = {}\n",
    "params['criterion'] = ['gini', 'entropy']\n",
    "params['n_estimators'] = (10, 20, 30)\n",
    "params['min_samples_split'] = np.arange(2,11,2)\n",
    "clf = HalvingGridSearchCV(RandomForestClassifier(), param_grid=params, n_jobs=-1, cv=cv, scoring=scoring)\n",
    "classifiers.append(['RF', clf, params])\n",
    "\n",
    "# LDA\n",
    "params = {}\n",
    "params['solver'] = ['svd']\n",
    "clf = HalvingGridSearchCV(LinearDiscriminantAnalysis(), param_grid=params, n_jobs=-1, cv=cv, scoring=scoring)\n",
    "classifiers.append(['LDA', clf, params])\n",
    "\n",
    "# SVM\n",
    "params = {}\n",
    "params['C'] = (1e-4, 1e-2, 1)\n",
    "params['gamma'] = (1e-4, 1e-2, 1, 10)\n",
    "params['kernel'] = ['linear', 'rbf']\n",
    "clf = HalvingGridSearchCV(SVC(), param_grid=params, n_jobs=-1, cv=cv, scoring=scoring)\n",
    "classifiers.append(['SVM', clf, params])\n",
    "\n",
    "# SGD\n",
    "params = {}\n",
    "params['loss'] = ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron']\n",
    "params['penalty'] = ['l2', 'l1', 'elasticnet']\n",
    "params['alpha'] = (1e-4, 1e-2, 1, 10)\n",
    "clf = HalvingGridSearchCV(SGDClassifier(), param_grid=params, n_jobs=-1, cv=cv, scoring=scoring)\n",
    "classifiers.append(['SGD', clf, params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4fdb16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train Classifiers\n",
    "for c in range(len(classifiers)):\n",
    "    clf_name = classifiers[c][0]\n",
    "    print(\"Training %s...\" % clf_name)\n",
    "    clf = classifiers[c][1].fit(X, y)\n",
    "    print('%s score: %2.2f' % (clf_name, clf.best_score_))\n",
    "    print('%s std  : %2.2f' % (clf_name, np.mean(clf.cv_results_['std_test_score'])))\n",
    "    print()\n",
    "    scores['Classifier'].append(clf_name)\n",
    "    scores['Score'].append(clf.best_score_)\n",
    "    scores['Std'].append(np.mean(clf.cv_results_['std_test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6522303",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Score summary\n",
    "df = pd.DataFrame(scores)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6adb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Classifier\n",
    "print('Best Classifier:')\n",
    "df.loc[df['Score'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e4d918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
